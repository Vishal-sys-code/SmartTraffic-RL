Metadata-Version: 2.4
Name: SmartTraffic-RL
Version: 0.1.0
Summary: A Gym-style RL environment for adaptive urban traffic signal control.
Home-page: https://github.com/openai/smart-traffic-rl
Author: Vishal Pandey
Author-email: pandeyvishal.mlprof@gmail.com
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: gymnasium>=0.26.0
Requires-Dist: numpy>=1.19.0
Dynamic: author
Dynamic: author-email
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# SmartTraffic-RL
A Gym-style RL environment and benchmark for adaptive urban traffic signal control using macroscopic flow models.

## Architecture Overview

Below is the end-to-end structure of **SmartTraffic-RL**, showing how the components interact:
![Architecture](docs/architecture.png)

## Getting Started

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-repo/SmartTraffic-RL.git
   cd SmartTraffic-RL
   ```

2. Install the package in editable mode with its dependencies:
   ```bash
   pip install -e .
   ```
   You will also need to install `stable-baselines3` with PyTorch. It is recommended to install the CPU-only version of PyTorch if you are not using a GPU.
   ```bash
   pip install "stable-baselines3[extra]" torch --index-url https://download.pytorch.org/whl/cpu
   ```

### Training a PPO Agent

The `examples/train_ppo.py` script provides a configurable way to train a PPO agent. It includes observation normalization and reward scaling, which are crucial for stable training.

To train an agent with a custom set of hyperparameters:
```bash
python examples/train_ppo.py \
    --exp_name "my_experiment" \
    --lr 1e-4 \
    --n_steps 4096 \
    --reward_scale 1e6 \
    --total_timesteps 200000
```
Training logs and the best model will be saved in the `logs/` and `ppo_tensorboard/` directories.

### Evaluating a Trained Agent

The `examples/evaluate.py` script allows you to evaluate a trained model and compare its performance against a fixed-time baseline.

To run an evaluation:
```bash
python -m examples.evaluate --model_path /path/to/your/model.zip
```
This will print the average queue length for both the PPO agent and the fixed-time agent over 10 episodes.

## Results

After implementing observation/reward scaling and tuning hyperparameters, the PPO agent successfully learns a policy that outperforms a fixed-time (equal green splits) baseline.

The following results were obtained after training for 200,000 timesteps with a learning rate of `1e-4`, `n_steps` of `4096`, and a reward scaling factor of `1e6`:

| Agent      | Average Queue Length |
|------------|----------------------|
| PPO (Tuned)| **246.07**           |
| Fixed-Time | 246.95               |

The training metrics showed a healthy learning process, with `approx_kl` around `0.01` and `explained_variance` consistently above 0, indicating that the value function was learning effectively. This confirms that the PPO pipeline is working as expected.
